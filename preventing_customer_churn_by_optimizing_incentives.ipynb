{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Customer Churn by Optimizing Incentive Programs\n",
    "\n",
    "----\n",
    "## Table of contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Exploration](#Data-Exploration)\n",
    "4. [Model Training](#Train)\n",
    "6. [Model Hosting](#Host)\n",
    "7. [Assess and optimize](#Assess-and-optimize)\n",
    "8. [Summary](#Summary)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "_This notebook has been adapted from [Amazon SageMaker Examples - Customer Churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn). That notebook had been adapted from the [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/) and [AWS notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)._\n",
    "\n",
    "*This notebook also builds on the AWS blog [Gain customer insights using Amazon Aurora machine learning](https://aws.amazon.com/blogs/machine-learning/gain-customer-insights-using-amazon-aurora-machine-learning/), which focused on integrating churn information into the customer service response process, offering selected customers an incentive.  For ease of experimentation, this notebook has been built to run stand-alone, but the methods developed here are intended for integration into the environment of the prior blog.*\n",
    "\n",
    "*In this version of the notebook, in addition to building the predictive model we explore the key question: How do we create an optimal incentive program that we (as the provider) think is most likely to reduce churn, with the minimum cost to us?*\n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early on gives the business a chance to offer them incentives to stay.  This notebook describes using machine learning (ML) for the automated identification of unhappy customers, also known as customer churn prediction. ML models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML.\n",
    "\n",
    "We use an example of churn that is familiar to all of us â€“ leaving a mobile phone operator.  Seems like I can always find fault with my provider du jour! And if my provider knows that Iâ€™m thinking of leaving, it can offer timely incentives â€“ I can always use a phone upgrade or perhaps have a new feature activated â€“ and I might just stick around. Incentives are often much more cost effective than losing and reacquiring a customer.\n",
    "\n",
    "*The first sections of the notebook are identical to the source XGBoost notebook. Substantial differences begin at the heading, \"Assessing business impact...\"*\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an __ml.m4.xlarge__ notebook instance._\n",
    "\n",
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "# To run predictions against the model \n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Data manipulations:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# To mix code and markdown\n",
    "from IPython.display import Markdown\n",
    "# from ColorBrewer\n",
    "plot_color = \"#4daf4a\"\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountid = boto3.client('sts').get_caller_identity()['Account']\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "# provide a prefix to be attached to the output files in the bucket\n",
    "prefix = 'sagemaker/xgboost-churn-ecooptimize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Session bucket\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operatorâ€™s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakesâ€“after all, predicting the future is tricky business! But Iâ€™ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"churn.txt\"):\n",
    "    !aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./\n",
    "else:\n",
    "    print(\"File has been already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modern standards, itâ€™s a relatively small dataset, with only 3,333 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customerâ€™s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Intâ€™l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attributeâ€“the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "Preview the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the customer churn data to pandas DataFrame\n",
    "pd.set_option('display.max_columns', 25)\n",
    "churn = pd.read_csv('./churn.txt')\n",
    "\n",
    "# review the top rows\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data\n",
    "\n",
    "Let's begin exploring the data. \n",
    "\n",
    "_This section is identical to the original notebook, [Amazon SageMaker Examples - Customer Churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn). While data exploration is an important topic, it's not the focus of this walk through. Therefore it's been removed in the interests of brevity, but the actions taken based on the analysis have been kept (i.e., columns kept/removed). Please refer to the original notebook for this section._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll save a copy of the original dataset, for use later\n",
    "churn_save = churn.copy()\n",
    "# Then, we'll add a column for the total customer spend\n",
    "churn_save['Total Customer Spend'] = churn_save.apply(lambda x: x['Day Charge'] + x['Night Charge'] + x['Eve Charge'] \n",
    "                                                      + x['Intl Charge'], axis=1)\n",
    "churn_save['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's determine which algorithm to use.  As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms.  Instead, let's attempt to model this problem using gradient boosted trees.  Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format.  For this example, we'll stick with CSV.  It should (documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)):\n",
    "\n",
    "- Have the predictor variable in the first column\n",
    "- Not have a header row\n",
    "\n",
    "Since the model will not have the feature names later, when we explore the results, we will need to assign them from the original data (excluding the target variable)\n",
    "\n",
    "But first, let's convert our categorical features into numeric features as the algorithm manages only numeric features. Then, we place the outcome as the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's split the data into training, validation, and test sets.  This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen.\n",
    "\n",
    "_Note that different splits of the data may create slightly different results. In addition, on different runs against the same data, XGBoost may choose different combinations of features and trees that give similar model performance._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac = 1, random_state = 1729), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header = False, index = False)\n",
    "validation_data.to_csv('validation.csv', header = False, index = False)\n",
    "test_data.to_csv('test.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_columns=test_data.columns\n",
    "test_data_columns\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers.\n",
    "\n",
    "_This section is identical to the original notebook, [Amazon SageMaker Examples - Customer Churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn)._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, '1.0-1')\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters. \n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    serializer=CSVSerializer(),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge', serializer = CSVSerializer(), endpoint_name='prevent-churn-oda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "_This section has minor changes from the original notebook, to set up for the next section. We save the input file, and then add the predictions as a column, so that all customer data is available to us in addition to the prediction._  \n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simpler traceability to our original dataset given our focus on optimization rather than on improving the ML model, we'll take a section of our saved dataset and use that to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500 \n",
    "\n",
    "churn_sample = churn_save.sample(N)\n",
    "# Convert this subset to dummies for use in inference; but remove columns that may cause dimension explosion\n",
    "# Note that dummies can be problematic, as all categorical variables must be represented as with the full dataset.\n",
    "churn_sample_dummies = pd.get_dummies(churn_sample.drop(['Phone', 'Total Customer Spend'], axis=1))\n",
    "churn_sample_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of 'predict' allows us to pass a dataset with more columns, and a list of the columns to be used in the prediction\n",
    "def predict_cost(data, columns, rows=500):\n",
    "    test_data = data[columns]\n",
    "    test_data_nolab = test_data.values[:, 1:]\n",
    "    split_array = np.array_split(test_data_nolab, int(test_data_nolab.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict_cost(churn_sample_dummies, test_data.columns)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_sample['Churn Probability'] = predictions\n",
    "churn_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Assess and optimize\n",
    "\n",
    "_This section contains the new material, and is the focus of this blog post._  \n",
    "\n",
    "We can assess the model performance by looking at the prediction scores, as shown in the original post, [Amazon SageMaker Examples - Customer Churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn). \n",
    "\n",
    "While itâ€™s usual to treat this as a binary classification problem (â€˜1â€™ or â€˜0â€™), in fact, the real world is less binary: people become â€œlikely to churnâ€ for some time before they actually churn. Loss of â€œbrand loyaltyâ€ occurs some time before someone actually buys from a competitor. There's frequently a slow rise in dissatisfaction over time before someone is finally driven to act. Providing the right incentive at the right time can reset a customer's satisfaction.\n",
    "\n",
    "So how do calculate the minimum incentive that will give the desired result? Rather than providing a single program to all customers, can we save money and gain a better outcome by using variable incentives, customized to a customer's churn probability and value? And if so, how?\n",
    "\n",
    "We can do so by building on components we've already developed so far.\n",
    "\n",
    "#### Assigning costs to our predictions\n",
    "\n",
    "What are the costs for our problem of mobile operator churn? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.\n",
    "\n",
    "First, we'll assign the true negatives the cost of \\\\$0. Our model essentially correctly identified a happy customer in this case, and we wonâ€™t offer them an incentive. An alternative is to assign the true negatives the actual value of the customer's spend, as this is the customer's contribution to our overall revenue. \n",
    "\n",
    "False negatives are the most problematic, because they incorrectly predict that a churning customer will stay. We lose the customer and will have to pay all the costs of acquiring a replacement customer, including foregone revenue, advertising costs, administrative costs, point of sale costs, and likely a phone hardware subsidy. A quick search on the Internet reveals that such costs typically run in the hundreds of dollars so, for the right now, let's assume $500. This is the cost we'll use for each false negative. Our marketing department should be able to give us a value to use here for the overhead, and we have the actual customer spend for each customer in our dataset. \n",
    "\n",
    "Finally, we'll give an incentive to customers that our model identifies as churning. At this point let's assume a one-time retention incentive in the amount of \\$50. This is the cost we'll apply to both true positive and false positive outcomes. In the case of false positives (the customer is happy, but the model mistakenly predicted churn), we will â€œwasteâ€ the concession. We probably could have spent those dollars more effectively, but it's possible we increased the loyalty of an already loyal customer, so thatâ€™s not so bad. We'll be revising this initial approach below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the continuous values of our churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.xlabel('Churn prediction score')\n",
    "plt.ylabel('Number of customers')\n",
    "plt.title('Prediction Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the customer churn threshold\n",
    "\n",
    "In previous versions of this notebook, we've shown the effect of false negatives that are substantially more costly than false positives. Instead of optimizing for error based on the number of customers, we've used a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "cost_of_replacing_customer * FN(C) + customer_value * TN(C) + incentive_offered * FP(C) + incentive_offered * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We'd like to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "Right now we'll start by using the same values for all customers, to give us a starting point for discussion with the business. With the estimates we'll use for right now, this equation becomes:\n",
    "\n",
    "```txt\n",
    "$500 * FN(C) + $0 * TN(C) + $50 * FP(C) + $50 * TP(C)\n",
    "```\n",
    "\n",
    "A straightforward way to understand the impact of these numbers is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "num_below_cutoff = []\n",
    "fn = 500\n",
    "tn = 0\n",
    "fp = 50\n",
    "tp = 50\n",
    "for c in cutoffs:\n",
    "    crsstb = pd.crosstab(index=churn_sample_dummies['Churn?_True.'],       \n",
    "                                           columns=np.where(predictions > c, 1, 0))\n",
    "    if crsstb.shape == (2,1):\n",
    "        print(crsstb.columns)\n",
    "        if crsstb.columns[0] == 0:   # Then we're missing the '1' column\n",
    "            crsstb[1] = 0\n",
    "        else:\n",
    "            crsstb[0] = 0\n",
    "    costs.append(np.sum(np.sum(np.array([[tn, tp], [fn, fp]]) * crsstb )))\n",
    "    num_below_cutoff.append(np.count_nonzero(np.where(predictions <= c, 1, 0)))\n",
    "    \n",
    "costs = np.array(costs)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(cutoffs, costs)\n",
    "fmt = '${x:,.0f}'\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick) \n",
    "ax.tick_params(axis='y', labelcolor='b')\n",
    "plt.xlabel('Threshold')\n",
    "ax.set_ylabel('Cost',color='b')\n",
    "\n",
    "ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Number of customers below cutoff')  \n",
    "ax2.plot(cutoffs, num_below_cutoff, color='k')\n",
    "ax2.tick_params(axis='y', labelcolor='k')\n",
    "\n",
    "plt.title('Cost versus Threshold')\n",
    "plt.show()\n",
    "dex = np.argmin(costs)\n",
    "incentives_paid_to = len(churn_sample_dummies) - num_below_cutoff[dex]\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[dex], 'for a cost of: $', np.min(costs), 'for these', len(predictions), 'customers.')\n",
    "print('Incentive is paid to', incentives_paid_to,'customers, for a total outlay of $', incentives_paid_to * tp)\n",
    "print('Total customer spend of these customers is $',\n",
    "          churn_sample[churn_sample['Churn Probability'] > cutoffs[dex]]['Total Customer Spend'].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows how picking a threshold too low results in costs skyrocketing as all customers are given a retention incentive.  Meanwhile, setting the threshold too high (e.g., 0.7 or above) results in too many lost customers, which ultimately grows to be nearly as costly. In between, there is a large \"grey\" area, where perhaps some more nuanced incentives would create better outcomes.  \n",
    "\n",
    "The overall cost can be minimized at \\\\$25800 by setting the cutoff to 0.21, which is substantially better than the \\\\$100k+ we would expect to lose by not taking any action.\n",
    "\n",
    "We can also calculate the dollar outlay of the program, and compare to the total spend of the customers. Here we can see that paying the incentive to all predicted churn customers will cost \\\\$12300, and that these customers spend \\\\$16324. (Your numbers may vary, depending on the specific customers randomly chosen for the sample.)  \n",
    "\n",
    "What happens if we instead have a smaller budget for our campaign? We'll choose a budget of 1% of total customer monthly spend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: Incentive Budget equal to a fixed %age of the Total revenue\n",
    "C = 0.01*np.sum(churn_sample['Total Customer Spend'].values) \n",
    "\n",
    "print('Total budget is:', '${:,.2f}'.format(C))\n",
    "incentive = C / N\n",
    "print('Per customer incentive is', '${:,.2f}'.format(incentive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "num_below_cutoff = []\n",
    "fn = 500\n",
    "tn = 0\n",
    "fp = incentive\n",
    "tp = incentive\n",
    "for c in cutoffs:\n",
    "    crsstb = pd.crosstab(index=churn_sample_dummies['Churn?_True.'],       \n",
    "                                           columns=np.where(predictions > c, 1, 0))\n",
    "    if crsstb.shape == (2,1):\n",
    "        print(crsstb.columns)\n",
    "        if crsstb.columns[0] == 0:   # Then we're missing the '1' column\n",
    "            crsstb[1] = 0\n",
    "        else:\n",
    "            crsstb[0] = 0\n",
    "    costs.append(np.sum(np.sum(np.array([[tn, tp], [fn, fp]]) * crsstb )))\n",
    "    num_below_cutoff.append(np.count_nonzero(np.where(predictions <= c, 1, 0)))\n",
    "    \n",
    "costs = np.array(costs)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(cutoffs, costs)\n",
    "fmt = '${x:,.0f}'\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick) \n",
    "ax.tick_params(axis='y', labelcolor='b')\n",
    "plt.xlabel('Threshold')\n",
    "ax.set_ylabel('Cost',color='b')\n",
    "\n",
    "ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Number of customers below cutoff')  \n",
    "ax2.plot(cutoffs, num_below_cutoff, color='k')\n",
    "ax2.tick_params(axis='y', labelcolor='k')\n",
    "\n",
    "plt.title('Cost versus Threshold')\n",
    "plt.show()\n",
    "\n",
    "dex = np.argmin(costs)\n",
    "incentives_paid_to = len(churn_sample_dummies) - num_below_cutoff[dex]\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[dex], 'for a cost of: $', np.min(costs), 'for these', len(predictions), 'customers.')\n",
    "print('Incentive is paid to', incentives_paid_to,'customers, for a total outlay of $', incentives_paid_to * tp)\n",
    "print('Total customer spend of these customers is $',\n",
    "          churn_sample[churn_sample['Churn Probability'] > cutoffs[dex]]['Total Customer Spend'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cost to us changes. But it's pretty clear that an incentive of ~\\\\$0.60 is unlikely to change many people's minds.   \n",
    "\n",
    "For better outcomes, we could even offer a range of incentives to customers that meet different criteria. For example, it's worth more to the business to prevent a high spend customer from churning than a low spend customer. We could also target the \"grey area\" of customers that have less loyalty and could be swayed by another company's advertising. Let's explore that now. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing customer churn using mathematical optimization of incentive programs \n",
    "\n",
    "Now let's use a more sophisticated approach to developing our customer retention program. We'd like to tailor our incentives to target the customers most likely to reconsider a \"churn\" decision. \n",
    "\n",
    "Intuitively, we know that we do not need to offer an incentive to customers with a low churn probability. Also, above some threshold, we've already lost the customer's heart and mind, even if they haven't actually left yet. So the best target for our incentive is between those two thresholds - these are the customers we can convince to stay. \n",
    "\n",
    "Let's formulate this as a mathematical optimization problem.\n",
    "\n",
    "The problem under investigation is inherently stochastic in that each customer might churn or not, and might accept the incentive (offer) or not. Stochastic programming [1, 2] is an approach for modeling optimization problems that involve uncertainty. Whereas deterministic optimization problems are formulated with known parameters, real world problems almost invariably include parameters which are unknown at the time a decision should be made. An example would be the construction of an investment portfolio to maximize return. An efficient portfolio would be defined as the portfolio that maximizes the expected return for a given amount of risk (e.g. standard deviation), or the portfolio that minimizes the risk subject to a given expected return [3].\n",
    "\n",
    "References: [1] S. Uryasev, P. M. Pardalos, Stochastic Optimization: Algorithm and Applications, Kluwer Academic: Norwell, MA, USA, 2001.\n",
    "[2] John R. Birge and FranÃ§ois V. Louveaux. Introduction to Stochastic Programming. Springer Verlag, New York, 1997.\n",
    "[3] Francis, J. C. and Kim, D. (2013). Modern portfolio theory: Foundations, analysis, and new developments (Vol. 795). John Wiley & Sons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    N & : \\text{number of customers}\n",
    "    \\\\\n",
    "    i & \\in \\{1,\\ldots,N\\}\n",
    "    \\\\\n",
    "    P_i & : \\text{profit generated by customer i}\n",
    "    \\\\\n",
    "    \\alpha_i & : \\text{probability customer i will churn}\n",
    "    \\\\\n",
    "    c_i & : \\text{discount or incentive to be offered to customer i}\n",
    "    \\\\\n",
    "    C & = \\sum^{N}_{i=1}c_i,  \\text{ total retention campaign budget}\n",
    "    \\\\\n",
    "    \\gamma_i & \\in (0,1), \\text{convincing factor for customer i}\n",
    "    \\\\\n",
    "    \\beta_i & = 1-e^{-\\gamma_i c_i}, \\text{ probability customer i will accept the discount $c_i$}\n",
    "    \\\\\n",
    "    f(c_i) & = \\sum^{N}_{i=1} P_i(1-\\alpha_i) + \\sum^{N}_{i=1} \\beta_i(\\alpha_i P_i - c_i), \\text{expected total profit}\n",
    "\\end{align}\n",
    "\n",
    "Our goal is to optimally allocate the discount ğ‘ğ‘– across the ğ‘ customers in order to maximize the expected total profit. Mathematically this is equivalent to the following optimization problem:\n",
    "\\begin{aligned}\n",
    "& \\underset{c_i}{\\text{maximize}}\n",
    "& & f(c_i) \\\\\n",
    "& \\text{subject to}\n",
    "& & \\sum^{N}_{i=1}c_i \\leq C \n",
    "\\\\\n",
    "&&& c_i \\geq 0.\n",
    "\\end{aligned}\n",
    "\n",
    "For our situation:\n",
    "* We know the number of customers, N.\n",
    "* We can use their spend from their customer record as the (upper bound) estimate of the profit they generate, P.\n",
    "* We can use the churn score from our ML model as an estimate of the probability of churn, alpha. \n",
    "* The incentive, c, is what we'd like to calculate.\n",
    "* We'll use 1% of our total revenue as our campaign budget, C.\n",
    "* The probability that the customer will be swayed, beta, depends on how convincing the incentive is to the customer - which we've represented as $\\gamma$.\n",
    "\n",
    "That leaves $\\gamma$, the convincing factor to be defined, below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up our inputs: P, profit; alpha, our churn probabilities, from our model above; and C, our campaign budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P: vector of the total customer spend\n",
    "P = churn_sample['Total Customer Spend'].values\n",
    "# alpha: vector of churn probabilities\n",
    "alpha = churn_sample['Churn Probability'].values\n",
    "\n",
    "print('Total budget is:', '${:,.2f}'.format(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add a variable (gamma) that allows us to specify how likely we think each customer is to accept the offer and not churn - that is, how convincing they find the incentive. \n",
    "\n",
    "While this is a matter of business judgment, we can use the graph above to inform that judgment. In this case, the business believes that if the churn probability is below 0.55, they are unlikely to churn, even without an incentive; on the other hand, if the customer's churn probability is above 0.95, the customer has little loyalty and is unlikely to be convinced. The real target for the incentives are the customers with churn probability between 0.55 and 0.95. \n",
    "\n",
    "We could include that business insight into the optimization by setting the value for the convincing factor $\\gamma$ as follows:\n",
    "\n",
    "- $\\gamma_i$ = 100. This is equivalent to giving less importance as deciding factor to the discount $c_i$  for customers whose churn probability $\\alpha_i$ is below 0.55 (they are loyal and less likeley to churn) and/or greater than 0.95 (they will most likely leave despite the retention campaign)\n",
    "- $\\gamma_i$ = 1. This is equivalent to saying that the probability customer i will accept the discount $c_i$ is equal to $\\beta = 1-e^{-c_i}$) for customer whose $\\alpha_i$ $\\in$ [0.55, 0.95]\n",
    "\n",
    "Once we start to offer these incentives, we can log whether or not each customer accepts the offer and remains a customer. With that information, we can learn this function from experience, and use that learned function to develop the next set of incentives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.ones(N)\n",
    "len(np.where(alpha > 0.95)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_gamma_eq_zero = np.union1d(np.where(alpha > 0.95)[0], np.where(alpha < 0.55)[0])\n",
    "gamma[indices_gamma_eq_zero] = 100\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a variety of open source solvers available that can solve this optimization problem for us. Examples include SciPy scipy.optimize.minimize, or faster open source solvers like GEKKO (https://gekko.readthedocs.io/en/latest/), which is what we use here. For large-scale problems, we would recommend using commercial optimization solvers like CPLEX or GUROBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gekko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note! Due to the stochastic nature of the algorithm, it may occasionally not converge on a solution. In these cases it's often solved by running the algorithm again; or, as a last resort, slightly modifying the value of C has been found to help the algorithm find a solution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gekko import GEKKO\n",
    "m = GEKKO(remote=False)\n",
    "m.options.SOLVER = 3 #IPOPT Solver\n",
    "m.options.IMODE = 3\n",
    "\n",
    "#C=1000\n",
    "# variable array dimension\n",
    "# create array\n",
    "x = m.Array(m.Var,N)\n",
    "for i in range(N):\n",
    "    x[i].value = C / N\n",
    "    x[i].lower = 0\n",
    "    x[i].upper = 10000000\n",
    "    \n",
    "# create parameter\n",
    "budget = m.Param(value = C)\n",
    "ival_eq = [m.Intermediate(x[i]) for i in range(N)]\n",
    "#ival_eq_2 = [m.Intermediate(x[i]) for i in range(int(N/2),N)]\n",
    " \n",
    "m.Equation(sum(ival_eq)==budget)\n",
    "\n",
    "beta =  [1 - m.exp(-gamma[i] *  x[i]) for i in range(N)]\n",
    "ival = [m.Intermediate(beta[i] * (alpha[i] * P[i] - x[i])) for i in range(N)]\n",
    "#ival_2 = [m.Intermediate(beta[i] * (alpha[i] * P[i] - x[i])) for i in range(int(N/2),N)]\n",
    "m.Obj(-sum(ival))\n",
    "\n",
    "# minimize objective\n",
    "m.solve()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Gekko returns an array of arrays so transforming to array\n",
    "x = np.array([a[0] for a in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the budget constraint C is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total spend is', '${:,.2f}'.format(np.sum(x)), 'compared to our budget of', '${:,.2f}'.format(C))\n",
    "print('Total customer spend is', '${:,.2f}'.format(churn_sample['Total Customer Spend'].sum()), 'for', len(churn_sample), 'customers.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the expected total profit for the following scenarios:\n",
    "1. Optimal discount allocation, as calculated by our optimization algorithm\n",
    "2. Uniform discount allocation - every customer is offered the same incentive\n",
    "3. No discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_total_profit(x, gamma, alpha, P):\n",
    "    # beta: vector of probabilities customer will accept the offer\n",
    "    beta = 1  - np.exp(-gamma * (x))\n",
    "    \n",
    "    return np.sum(P * (1 - alpha)) + np.sum(beta * (alpha * P - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_total_profit_no_campaign = expected_total_profit(0, gamma, alpha, P)\n",
    "expected_total_profit_optimal = expected_total_profit(x, gamma, alpha, P)\n",
    "expected_total_profit_uniform_campaign = expected_total_profit((C/N)*np.ones(N), gamma, alpha, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "data = [expected_total_profit_optimal, expected_total_profit_uniform_campaign, expected_total_profit_no_campaign]\n",
    "labels = ['Optimised Campaign', 'Naive Uniform Spend', 'No Campaign']\n",
    "plt.xticks(range(len(data)), labels)\n",
    "plt.xlabel('Budget Allocation Approach')\n",
    "plt.ylabel('Expected Total Profit')\n",
    "# plt.title('Benefit of Optimisation with N=%i Customers' %N)\n",
    "plt.bar(range(len(data)), data)\n",
    "ax = plt.gca()\n",
    "fmt = '${x:,.0f}'\n",
    "formatter = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.savefig(\"Profits from optimisation\", transparent=True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Expected total profit compared to no campaign:  %.0f%%\" %(100*(expected_total_profit_optimal-expected_total_profit_no_campaign)/expected_total_profit_no_campaign))\n",
    "print(\"Expected total profit compared to uniform discount allocation:  %.0f%%\" %(100*(expected_total_profit_optimal-expected_total_profit_uniform_campaign)/expected_total_profit_uniform_campaign))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we add the discount to our customer data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_sample['Convincing Factor'] = gamma\n",
    "churn_sample['Optimal Discount'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_sample['Optimal Discount'].hist(bins=20)\n",
    "plt.axvline(x=C/N, linewidth=3, color='r')\n",
    "plt.xlabel('Discount in $')\n",
    "plt.ylabel('# of Subscribers Offered Discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this graph mirrors the histogram of churn probabilies, above: a large number of people are unlikely to churn, and they are offered a very small discount. A smaller number of people are likely to churn, and they are offered a larger discount.\n",
    "\n",
    "The vertical line shows the discount offered by a naive, uniform allocation of the budget across all customers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each customer we can see their total spend, and the optimal incentive to offer that customer. We can see that the discount varies by churn probability, and we're assured that the incentive campaign will fit within our budget.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_sample[['State', 'Area Code', 'Phone', 'Churn?', 'Total Customer Spend', 'Churn Probability', 'Optimal Discount', \n",
    "              'Convincing Factor']].sort_values(by='Optimal Discount', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_sample[['State', 'Area Code', 'Phone', 'Churn?', 'Total Customer Spend', 'Churn Probability', 'Optimal Discount', \n",
    "              'Convincing Factor']].sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Depending on the size of the total budget we allocate, we may occasionally find that weâ€™re offering all customers a discount. This discount allocation problem reminds us of the water-filling algorithm in wireless communications [4,5], where the problem is of maximizing the mutual information between the input and the output of a channel composed of several subchannels (such as a frequency-selective channel, a time-varying channel, or a set of parallel subchannels arising from the use of multiple antennas at both sides of the link) with a global power constraint at the transmitter. More power is allocated to the channels with higher gains to maximize the sum of data rates or the capacity of all the channels. The solution to this class of the problems can be interpreted by a vivid description as pouring limited volume of water into a tank, the bottom of which has the stair levels determined by the inverse of the sub-channel gains. \n",
    "\n",
    "Unfortunately our problem does not have an intuitive explanation as for the water-filling problem. This is due to the fact that, because of the nature of the objective function, the system of equations and inequalities corresponding to the KKT conditions [6] does not admit a closed form solution.\n",
    "\n",
    "The optimal incentives calculated here are the result of an optimization routine designed to maximize an economic figure, which is the expected total profit. While this approach provides a principled way for Marketing teams to make systematic, quantitative and analytics-driven decisions, it is also important to recall that the objective function to be optimized is a proxy measure to the actual total profit. It goes without saying that we cannot compute the actual profit based on future decisions (e.g. this would paradoxically imply maximizing the actual return based on future values of the stocks). But we can explore new ideas using techniques such as the potential outcomes work [7], which could be leveraged to design strategies for back-testing of our solution.\n",
    "\n",
    "\n",
    "References: [4] T. M. Cover and J. A. Thomas, Elements of Information Theory. New York: Wiley, 1991.\n",
    "[5] D. P. Palomar and J. R. Fonollosa, â€œPractical algorithms for a family of water-filling solutions,â€ IEEE Trans. Signal Process., vol. 53, no. 2, pp. 686â€“695, Feb. 2005.\n",
    "[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.\n",
    "[7] Imbens, G. W. and D. B. Rubin (2015): Causal Inference for Statistics, Social, and Biomedical Sciences, Cambridge University Press.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Weâ€™ve now taken another step towards preventing customer churn. Weâ€™ve built on the prior blog, where we integrated our customer data with our ML model to predict churn. We can now experiment with variations on this optimization equation, and see the effect of different campaign budgets or even different theories of how they should be modeled.\n",
    "\n",
    "To gather more data on effective incentives and customer behavior, we could also test several campaigns against different subsets of our customers. We can collect their responses â€“ do they churn after being offered this incentive, or not? â€“ and use that data in a future ML model to further refine the incentives offered. We can use this data to learn what kinds of incentives convince customers with different characteristics to stay, and then use that new function within this optimization.\n",
    "\n",
    "Now, weâ€™re empowering Marketing with the tools to make data-driven decisions that they can quickly turn into action. This approach can drive fast iterations on incentive programs, moving at the speed with which our customers make decisions. Over to you, Marketing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up\n",
    "\n",
    "If you're ready to be done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
